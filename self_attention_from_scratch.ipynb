{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMFllCPIsjeCPNoHr1Kj/AO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulaAdelKamal/Self-Attention-mechanism/blob/main/self_attention_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Self Attention from scratch using PyTorch\n",
        "\n",
        "In This notebook you will find the implementation for self attention mechanism from scrach"
      ],
      "metadata": {
        "id": "MFipHL0cTTXI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eBkAM3TRS2aI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements scaled dot-product self-attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        d_model (int): Dimension of input token embeddings and output projections.\n",
        "        row_dim (int): Dimension along which to compute similarity scores (default: 0).\n",
        "                       Typically corresponds to the sequence length dimension.\n",
        "        col_dim (int): Dimension along which to transpose keys for dot-product (default: 1).\n",
        "                       Typically corresponds to the embedding dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model=2, row_dim=0, col_dim=1):\n",
        "        super().__init__()\n",
        "        # Learnable projection matrices (no bias as in standard transformer architecture)\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)  # Query projection\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)  # Key projection\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)  # Value projection\n",
        "\n",
        "        # Configuration for flexible dimension handling (supports non-standard input shapes)\n",
        "        self.row_dim = row_dim  # Dimension for similarity score rows\n",
        "        self.col_dim = col_dim  # Dimension for similarity score columns\n",
        "\n",
        "    def forward(self, token_encoding):\n",
        "        \"\"\"\n",
        "        Compute attention-weighted contextualized token representations.\n",
        "\n",
        "        Args:\n",
        "            token_encoding (torch.Tensor): Input tensor of shape [..., seq_len, d_model]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Contextualized embeddings of shape [..., seq_len, d_model]\n",
        "\n",
        "        Shape:\n",
        "            Input: (..., S, E) where S is sequence length, E is embedding dimension\n",
        "            Output: (..., S, E) with same shape as input\n",
        "        \"\"\"\n",
        "        # Project input into query/key/value representations (same-dimensional space)\n",
        "        q = self.W_q(token_encoding)  # [..., S, E]\n",
        "        k = self.W_k(token_encoding)  # [..., S, E]\n",
        "        v = self.W_v(token_encoding)  # [..., S, E]\n",
        "\n",
        "        # Compute pairwise similarity scores between queries and keys\n",
        "        # Uses Einstein sum notation: sum over embedding dimension (E)\n",
        "        sims = torch.matmul(\n",
        "            q,\n",
        "            k.transpose(dim0=self.row_dim, dim1=self.col_dim)  # Swaps specified dimensions\n",
        "        )  # [..., S, S]\n",
        "\n",
        "        # Scale scores to prevent gradient saturation (sqrt(d_k) as in Transformer paper)\n",
        "        scaling_factor = torch.sqrt(torch.tensor(k.size(self.col_dim), dtype=torch.float32))\n",
        "        scaled_sim = sims / scaling_factor  # [..., S, S]\n",
        "\n",
        "        # Compute attention probabilities using softmax along sequence dimension\n",
        "        attention_percents = F.softmax(scaled_sim, dim=self.row_dim)  # [..., S, S]\n",
        "\n",
        "        # Compute weighted sum of value vectors using attention probabilities\n",
        "        attention_output = torch.matmul(attention_percents, v)  # [..., S, E]\n",
        "\n",
        "        return attention_output"
      ],
      "metadata": {
        "id": "CS5Kl_6KT6hE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Example Usage and Demonstration\n",
        "# ----------------------------\n",
        "\n",
        "# Synthetic input matrix representing 3 tokens with 2-dimensional embeddings\n",
        "# Shape: (sequence_length=3, d_model=2)\n",
        "encodings_matrix = torch.tensor(\n",
        "    [[1.16, 0.23],\n",
        "     [0.57, 1.36],\n",
        "     [4.41, -2.16]],  # Extreme values for demonstration of attention dynamics\n",
        "    dtype=torch.float32  # Explicit dtype for numerical stability\n",
        ")\n",
        "\n",
        "# Reproducibility setup for deterministic parameter initialization\n",
        "torch.manual_seed(139)  # Ensures consistent weight matrices across runs\n",
        "\n",
        "# Initialize self-attention module with configuration matching input dimensions\n",
        "# Note: row_dim=0 and col_dim=1 creates sequence-length vs feature dim attention\n",
        "selfAttention = SelfAttention(\n",
        "    d_model=2,        # Match input embedding dimension\n",
        "    row_dim=0,         # Sequence dimension for attention computation\n",
        "    col_dim=1          # Feature dimension for key transposition\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Forward Pass Execution\n",
        "# ----------------------------\n",
        "attention_output = selfAttention(encodings_matrix)  # Shape preserved: (3, 2)\n",
        "\n",
        "# The resulting tensor contains context-aware representations where:\n",
        "# - Each row corresponds to a token's updated embedding\n",
        "# - Columns maintain the original d_model dimension\n",
        "# - Gradients are enabled for subsequent backpropagation"
      ],
      "metadata": {
        "id": "cUSF0mQgVFxf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReAFjh5uXQxq",
        "outputId": "e67db37a-60b8-40cf-abd1-830b238f52d8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0129,  0.0187],\n",
              "        [-0.1479, -0.1828],\n",
              "        [ 0.3581,  0.4510]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run these commands in a Colab code cell\n",
        "!gh repo clone PaulaAdelKamal/Self-Attention-mechanism"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLhqS2LKb3E4",
        "outputId": "290b228d-2e1d-4467-ad25-1007c4767ce4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: gh: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/self attention from scratch.ipynb /content/your-repo/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xodd18BcIMj",
        "outputId": "7331f89d-61b8-4ec6-ffa8-da9d97849088"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: target '/content/your-repo/' is not a directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hhl7anZVcaT6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}